1.	We A/B tested two styles for a sign-up button on our company's product page. 100 visitors viewed page A, out of which 20 clicked on the button; whereas, 70 visitors viewed page B, and only 15 of them clicked on the button. Can you confidently say that page A is a better choice, or page B? Why?
 
	The only way to determine the better button would be to check how many clicks A had when its site was at 70 visitors, or wait to conduct the test once the site has had a total of 100 visitors while B was present. If the determination was made using only the click percentages B would the clear choice since it has a 2% higher click percentage than A (22% v. 20), but again this is based on differing base numerical values for each test. 
	For example, the next 30 people that visit the site while B is present may not click the button at all, and now A is the clear choice since it has a 5% higher click rate than B (20% v 15%). A better solution would be to have longer running tests for both buttons since such small sample sizes can often lead to vague, incorrect, or inconsistent conclusions.
 
 
2.	Can you devise a scheme to group Twitter users by looking only at their tweets? No demographic, geographic or other identifying information is available to you, just the messages they’ve posted, in plain text, and a timestamp for each message. Assuming you have a stream of these tweets coming in, describe the process of collecting and analyzing them, what transformations/algorithms you would apply, how you would train and test your model, and present the results.
 
	Start by inserting the entries into a new dictionary using the user_id as the key and the tweet as the value. Preprocess and normalize all the tweets removing stopwords and punctuation marks, stemming words, expanding acronyms, and reducing or eliminating repeat words. The features will then be extracted using engrams and bigrams to capture the words or phrases that will be used during feature selection. Create a new dictionary of the high frequency words or phrases. Train a K-Means clustering algorithm on the new dictionary and test it on newly captured data that has also been preprocessed. Present the results to the interested party in an visual that can be more easily understood than statistics.
 
 
3. In a classification setting, given a dataset of labeled examples and a machine learning model you're trying to fit, describe a strategy to detect and prevent overfitting.
 
	Detecting overfitting would begin by splitting the dataset into training, validation, and test sets. If the model is performing well on the training, but then failing to obtain the desired accuracy on the validation and test set it is more than likely that the training set is too small or to specific. A good way to address this would be to create a larger test set by skewing the items in the training set and adding them to the original dataset to increase the number of samples and variability in training set.
 
 
4. Your team is designing the next generation user experience for your flagship 3D modeling tool. Specifically, you have been tasked with implementing a smart context menu that learns from a modeler’s usage of menu options and shows the ones that would be most beneficial. E.g. I often use Edit > Surface > Smooth Surface, and wish I could just right click and there would be a Smooth Surface option just like Cut, Copy and Paste. Note that not all commands make sense in all contexts, for instance I need to have a surface selected to smooth it. How would you go about designing a learning system/agent to enable this behavior?
 
	A system of percentage counters could be developed. The setup would take into account what system is actually selected so that only the options that related to the system could be applied to each individually. Once an option is selected a count for that option in that system would be incremented and stored. The system would have a minimum threshold before it could add the feature as a favorite, and a percentage relation to the other options routinely selected to determine its status. For example an option has to be selected 3 times before it can be even be considered, but if it's used 3 times and another option is used 20 times, the option used 3 times may not break the percentage threshold of the option used 20 times and thus not considered a favorite. This issue could quickly get out of hand and isolate favorites if one option is used numerously while another is used somewhat frequently, so a cap on the times used would have to be considered to alleviate the pressure of high use options.
 
 
5. Give an example of a situation where regularization is necessary for learning a good model. How about one where regularization doesn't make sense?
 
	Regularization is used in situations where there are large discrepancies between multiple features used in training models. If the number of users in the tens of thousands and the times they use an app is in the tens or hundreds the weight given to the users could be disproportional to the weight given to the apps when used to train the model. By using regularization the extremes of features are brought in line with other features by penalizing the loss function. This technique can help avoid overfitting by better generalizing the data when making predictions. 
	A situation where regularization wouldn't be necessary could be a simple program that collects a small amount of features that is used by a very large population. As the user base grows, the need to regularize would decrease as the threat of overfitting diminishes with increased sample size.
 
 
6. Your neighborhood grocery store would like to give targeted coupons to its customers, ones that are likely to be useful to them. Given that you can access the purchase history of each customer and catalog of store items, how would you design a system that suggests which coupons they should be given? Can you measure how well the system is performing?
 
	Since you have access to both the customer purchases database and the store catalog database, determining what coupons to offer customers should be a relatively easy task. The system would be two fold. The first would determine categorically what products a customer buys and when since the frequency of how one item is purchased isn't necessarily relative to how other items are purchased (household good v produce). The system would then produce coupons dependant on the what time during a purchasing period the customer shops at the store. The second would keep track of the coupons delivered and the coupons used. While no coupons would ever be excluded from common customer purchases, the coupons that are used more frequently over time would have a higher rate of being printed as those are obviously what draws the customer to the store or are of the items that have the highest priority to the shopper. There could be a third step to this system that would suggest similar type good of different brands determined by what the store itself is trying to push, but I think that is outside the scope of this problem.
 
 
7. If you were hired for your machine learning position starting today, how do you see your role evolving over the next year? What are your long-term career goals, and how does this position help you achieve them?
 
	I would become more involved with deep learning program, assisting the lead directly until he/she is confident that I am capable of handling aspects of the project on my own, at which point I could start becoming more specialized in my path. Long-term I'd like to alternate between being a lead my own projects and working under others to so I can continue to expand my skills as a developer and become more comfortable teaching and guiding others. It's important to not become too comfortable in a role. Anyone's professional landscape can be turned upside down in an instant by factors totally outside of their control Learning to be flexible and handle a certain amount of discomfort and uncertainty can help weather stressful situations that are bound to arise sooner or later.
 
	
