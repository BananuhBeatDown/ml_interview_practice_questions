1.	We A/B tested two styles for a sign-up button on our company's product page. 100 visitors viewed page A, out of which 20 clicked on the button; whereas, 70 visitors viewed page B, and only 15 of them clicked on the button. Can you confidently say that page A is a better choice, or page B? Why?

	The only way to determine the better button would be to check how many clicks A had when its site was at 70 visitors, or wait to conduct the test once the site has had a total of 100 visitors while B was present. If the determination was made using only the click percentages B would the clear choice since it has a 2% higher click percentage than A (22% v. 20), but again this is based on differening base numerical values for each test. 
	For example, the next 30 people that visit the site while B is present may not click the button at all, and now A is the clear choice since it has a 5% higher click rate than B (20% v 15%). A better solution would be to have longer running tests for both buttons since such small sample sizes can often lead to vague, incorrect, or inconsistent conclusions.


2.	Can you devise a scheme to group Twitter users by looking only at their tweets? No demographic, geographic or other identifying information is available to you, just the messages theyâ€™ve posted, in plain text, and a timestamp for each message. Assuming you have a stream of these tweets coming in, describe the process of collecting and analyzing them, what transformations/algorithms you would apply, how you would train and test your model, and present the results.

	Start by inserting the entries into a new dictionary using the user_id as the key and the tweet as the value. Preprocess and normalize all the tweets removing stop words and punctuation marks, stemming words, expanding acronyms, and reducing or eliminating repeat words. The features will then be extracted using engrams and bigrams to capture the words or phrases that will be used during feature selection. Create a new dictionary of the high frequency words or phrases. Train a K-Means clustering algorithm on the new dictionary and test it on newly captured data that has also been preprocessed. Present the results to the interested party in an visual that can be more easliy understood than statistics.


3. In a classification setting, given a dataset of labeled examples and a machine learning model you're trying to fit, describe a strategy to detect and prevent overfitting.

	Detecting overfitting would begin by splitting the dataset into training, validation, and test sets. If the model is performing well on the training, but then failing to obtain the desired accuracy on the validation and test set it is more than likely that the training set is to small or to specific. A good way to address this would be to create a larger test set by skewing the items in the training set and adding them to the original dataset to increase the number of samples and variability in training set.

