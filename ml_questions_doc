1. We A/B tested two styles for a sign-up button on our company's product page. 100 visitors viewed page A, out of which 20 clicked on the button; whereas, 70 visitors viewed page B, and only 15 of them clicked on the button. Can you confidently say that page A is a better choice, or page B? Why?
 
	A simple A/B test will help our client, Linkedin, determine where to get the most bang for their buck. Website A, a the traditional sign-in page, has a current click probability of 20%, and the new sign-up page with interactive menus has a probability of roughly 15%. By Using the standard error calculation with a 95% confidence interval the conversion ranges are: site A 12%-28%, site B 12%-31%. These ranges overlap almost entirely so there is no way of knowing which is superior in this situation. However, when reducing the confidence interval to 80%, which is common practice when perfroming website conversion, the conversions rate change to: site A 15%-25%, site B 15%-28. Linkedin is again left with overlapping ranges, and are unable to determine which site is superior. Perhaps these sites need more trial time, or we can conclude that the differnce between the two is negligible and should use the one best fits with our current technology and/or business strategy.
 
 
2. Can you devise a scheme to group Twitter users by looking only at their tweets? No demographic, geographic or other identifying information is available to you, just the messages they’ve posted, in plain text, and a timestamp for each message. Assuming you have a stream of these tweets coming in, describe the process of collecting and analyzing them, what transformations/algorithms you would apply, how you would train and test your model, and present the results.
 
	Start by inserting the entries into a new dictionary using the user_id as the key and the tweet as the value. Preprocess and normalize all the tweets removing stopwords and punctuation marks, stemming words, expanding acronyms, and reducing or eliminating repeat words. The features will then be extracted using engrams and bigrams to capture the words or phrases that will be used during feature selection. Create a new dictionary of the high frequency words or phrases. Train a K-Means clustering algorithm on the new dictionary and test it on newly captured data that has also been preprocessed. Present the results to the interested party in an visual that can be more easily understood than statistics.
 
 
3. In a classification setting, given a dataset of labeled examples and a machine learning model you're trying to fit, describe a strategy to detect and prevent overfitting.
 
	Detecting overfitting is as simple as setting aside a portion of the dataset in order to validate the training results. If the model has high training accuracy and low validation accuracy there is a high probability of overfitting.
	There are many ways to prevent overfitting in machine learning models. The first and simplist would be to have a large, diverse dataset. Over complexity can also cause overfitting due to the data being so specific that the model loses the ability to generalize. This can be solved by reducing the model complexity, which can be accomplished in two ways. The first is with regularization, which is a process of penalizing the loss function of complex models as to avoid training the model on specifics rather than generalities. The second is by reducing the complexity by manually reducing the dimensionality of the model. Adding Dropout layers in the midst of a neural network can also reduce overfitting by temporarily removing random samples along with their inputs and outputs from the network to ensure it is unable to train on the same data continuously. 
 
 
4. Your team is designing the next generation user experience for your flagship 3D modeling tool. Specifically, you have been tasked with implementing a smart context menu that learns from a modeler’s usage of menu options and shows the ones that would be most beneficial. E.g. I often use Edit > Surface > Smooth Surface, and wish I could just right click and there would be a Smooth Surface option just like Cut, Copy and Paste. Note that not all commands make sense in all contexts, for instance I need to have a surface selected to smooth it. How would you go about designing a learning system/agent to enable this behavior?
 
	A system of percentage counters could be developed. The setup would take into account what system is actually selected so that only the options that related to the system could be applied to each individually. Once an option is selected a count for that option in that system would be incremented and stored. The system would have a minimum threshold before it could add the feature as a favorite, and a percentage relation to the other options routinely selected to determine its status. For example an option has to be selected 3 times before it can be even be considered, but if it's used 3 times and another option is used 20 times, the option used 3 times may not break the percentage threshold of the option used 20 times and thus not considered a favorite. This issue could quickly get out of hand and isolate favorites if one option is used numerously while another is used somewhat frequently, so a cap on the times used would have to be considered to alleviate the pressure of high use options.
 
 
5. Give an example of a situation where regularization is necessary for learning a good model. How about one where regularization doesn't make sense?
 
	Regularization is used in situations where there are large discrepancies between multiple features used in training models. If the number of users in the tens of thousands and the times they use an app is in the tens or hundreds the weight given to the users could be disproportional to the weight given to the apps when used to train the model. By using regularization the extremes of features are brought in line with other features by penalizing the loss function. This technique can help avoid overfitting by better generalizing the data when making predictions. 
	A situation where regularization wouldn't be necessary could be a simple program that collects a small amount of features that is used by a very large population. As the user base grows, the need to regularize would decrease as the threat of overfitting diminishes with increased sample size.
 
 
6. Your neighborhood grocery store would like to give targeted coupons to its customers, ones that are likely to be useful to them. Given that you can access the purchase history of each customer and catalog of store items, how would you design a system that suggests which coupons they should be given? Can you measure how well the system is performing?
 
	Since you have access to both the customer purchases database and the store catalog database, determining what coupons to offer customers should be a relatively easy task. The system would be two fold. The first would determine categorically what products a customer buys and when since the frequency of how one item is purchased isn't necessarily relative to how other items are purchased (household good v produce). The system would then produce coupons dependant on the what time during a purchasing period the customer shops at the store. The second would keep track of the coupons delivered and the coupons used. While no coupons would ever be excluded from common customer purchases, the coupons that are used more frequently over time would have a higher rate of being printed as those are obviously what draws the customer to the store or are of the items that have the highest priority to the shopper. There could be a third step to this system that would suggest similar type good of different brands determined by what the store itself is trying to push, but I think that is outside the scope of this problem.
 
 
7. If you were hired for your machine learning position starting today, how do you see your role evolving over the next year? What are your long-term career goals, and how does this position help you achieve them?
 
	I would become more involved with deep learning program, assisting the lead directly until he/she is confident that I am capable of handling aspects of the project on my own, at which point I could start becoming more specialized in my path. Long-term I'd like to alternate between being a lead my own projects and working under others to so I can continue to expand my skills as a developer and become more comfortable teaching and guiding others. It's important to not become too comfortable in a role. Anyone's professional landscape can be turned upside down in an instant by factors totally outside of their control Learning to be flexible and handle a certain amount of discomfort and uncertainty can help weather stressful situations that are bound to arise sooner or later.
 
	
