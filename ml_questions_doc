1. We A/B tested two styles for a sign-up button on our company's product page. 100 visitors viewed page A, out of which 20 clicked on the button; whereas, 70 visitors viewed page B, and only 15 of them clicked on the button. Can you confidently say that page A is a better choice, or page B? Why?
 
A simple A/B test will help our client, Linkedin, determine where to get the most bang for their buck. Website A, a the traditional sign-in page, has a current click probability of 20%, and the new sign-up page with interactive menus has a probability of roughly 15%. By Using the standard error calculation with a 95% confidence interval the conversion ranges are: site A 12%-28%, site B 12%-31%. These ranges overlap almost entirely so there is no way of knowing which is superior in this situation. However, when reducing the confidence interval to 80%, which is common practice when performing website conversion, the conversions rate change to: site A 15%-25%, site B 15%-28. Linkedin is again left with overlapping ranges, and are unable to determine which site is superior. Perhaps these sites need more trial time, or we can conclude that the difference between the two is negligible and should use the one best fits with our current technology and/or business strategy.
 
 
2. Can you devise a scheme to group Twitter users by looking only at their tweets? No demographic, geographic or other identifying information is available to you, just the messages they’ve posted, in plain text, and a timestamp for each message. Assuming you have a stream of these tweets coming in, describe the process of collecting and analyzing them, what transformations/algorithms you would apply, how you would train and test your model, and present the results.
 	
Using sentiment analysis on the stream of tweets for suicide prevention would be viable. Start by filtering the tweets by their timestamp. Anything older than a month will be ignored since the model is trying to identify people who are in imminent danger of committing suicide. Inserting the entries into a new dictionary using the user_id as the key and add a vector that contains all of that specific user's tweets as the value. Preprocess and normalize all the tweets removing stopwords and punctuation marks, stemming words, expanding acronyms, and reducing or eliminating repeat words. The features will then be extracted using engrams to search for negative trigger words such as 'depression', 'desperate', and 'hopeless' and bigrams to capture phrases linked to the word 'myself' such as 'hurt', 'hang', 'shoot', etc. The data can then be training using a SVM to determine what groups of tweets meet the criteria of negative self-talk and threats of suicide since there are many situations where one or the other could come up quite often: "Depressed new lipstick is not red as advertised", "Atlanta Falcons just blew 20 point lead. Going to kill myself". When a user is identified as a threat to themselves it would be reported to the national suicide prevention center where they could get in contact with the person through twitter offering services and contact information on how to get in touch with people and organizations that can help prevent a potential suicide.
 
 
3. In a classification setting, given a dataset of labeled examples and a machine learning model you're trying to fit, describe a strategy to detect and prevent overfitting.
 
Detecting overfitting is as simple as setting aside a portion of the dataset in order to validate the training results. If the model has high training accuracy and low validation accuracy there is a high probability of overfitting.
There are many ways to prevent overfitting in machine learning models. The first and simplest would be to have a large, diverse dataset. Over complexity can also cause overfitting due to the data being so specific that the model loses the ability to generalize. This can be solved by reducing the model complexity, which can be accomplished in two ways. The first is with regularization, which is a process of penalizing the loss function of complex models as to avoid training the model on specifics rather than generalities. The second is by reducing the complexity by manually reducing the dimensionality of the model. Adding Dropout layers in the midst of a neural network can also reduce overfitting by temporarily removing random samples along with their inputs and outputs from the network to ensure it is unable to train on the same data continuously. 
 
 
4. Your team is designing the next generation user experience for your flagship 3D modeling tool. Specifically, you have been tasked with implementing a smart context menu that learns from a modeler’s usage of menu options and shows the ones that would be most beneficial. E.g. I often use Edit > Surface > Smooth Surface, and wish I could just right click and there would be a Smooth Surface option just like Cut, Copy and Paste. Note that not all commands make sense in all contexts, for instance I need to have a surface selected to smooth it. How would you go about designing a learning system/agent to enable this behavior?
 
A reinforcement learning system could be developed to determine favorites in a program such as Blender. The setup would take into account what specific option is selected for a specific function so only the options related to that function would be considered when developing favorites. Once a option of a function is selected a count for that option would be incremented and stored. The system would have a minimum threshold before it could add the feature as a favorite, and a percentage relation to the other options routinely selected to determine its status. For example an option has to be selected 3 times before it can be even be considered, but if it's used 3 times and another option is used 20 times, the option used 3 times may not break the percentage threshold of the option used 20 times and thus not considered a favorite. This issue could quickly get out of hand and isolate favorites if one option is used numerously while another is used somewhat frequently, so a cap on the times used would have to be considered to alleviate the pressure of high use options. If the user determines that a new option is preferred rather than an existing favorited option, a feature could be built in to reset the favorites either entirely or individually. 
 
 
5. Give an example of a situation where regularization is necessary for learning a good model. How about one where regularization doesn't make sense?
 
Car companies could implement L2 regularization when using a classification model to determine which accidents resulting in fatalities. There are likely to be many different factors in each accident so the risk of the model eventually overfitting the data is high. Adding a regularization term, in this case L2 regularization, which is the sums of the squared weights multiplied by gamma, would help to minimize the extreme coefficients that could lead our model to overfit. In this way, the model is able to to continue minimizing loss while maintaining it's ability to generalize.
A situation where regularization wouldn't be necessary is a classification model that has a very large sample size for training, with few features, and near zero background noise so the model won't have the opportunity to overfit, and in actuality can't overfit the data.

 
6. Your neighborhood grocery store would like to give targeted coupons to its customers, ones that are likely to be useful to them. Given that you can access the purchase history of each customer and catalog of store items, how would you design a system that suggests which coupons they should be given? Can you measure how well the system is performing?
 
Since Harris Teeter has access to both the customer purchases database and the store catalog database, a recommender system using collaborative filtering based on customer's coupons preference and use habits can be implemented. First determine categorically what products a customer buys and when since the frequency of how one item is purchased isn't necessarily relative to how other items are purchased (household good v produce). The system could then determine what type of coupons would be most effective depending on the what time during a purchasing period the customer shops at the store, and also track the coupons delivered and the coupons used. It would being with printing coupons for the common goods that are frequently purchased. If the customer has no rate of using those coupons then the best the algorithm can could do is begin to print coupons for similar goods or goods that are often purchased in association with their commonly purchased goods. If the customer still refuses to use those coupons, then the system would determine how much a customer usually spends in the store, and print a coupon that will give the customer a certain percentage discount if they spend an amount over what they usually spend. However, if a customer did use coupons frequently, the system could begin tailoring certain coupons to that customer to influence their purchasing habits: suggesting higher-end or higher-markup items, or items purchased in conjunction at a discount, and hopefully foster new brand loyalty to a product that is more profitable to the store. 
 
 
 
7. If you were hired for your machine learning position starting today, how do you see your role evolving over the next year? What are your long-term career goals, and how does this position help you achieve them?
 
Yembo is company that is working to disrupt the entrenched remodeling business model that has been on the verge of a technological and visual breakthrough for a long time. This is what I found attractive from the first time I researched them. 
It seems like the the remodeling world has always wanted to move forward in terms of visual technology. Lowe's and Home Depot give their customers a chance to look at what options of woods and paints they could use in their homes on in-store computers. Pottery Barn and West Elm's websites use a similar system to give customers a chance to look at their store's different offerings in multiple varieties at their leisure. These are all steps in the right direction, but none of these platforms have unlocked the true potential that machine learning coupled with computer vision would bring to the industry.
As a machine learning engineer still earlier in my career, my passions have led me towards deep learning. I have built and trained multiple convolutional neural networks while working on image recognition and classification projects. During this time. One standout memory was training a network not only to recognize the digits within images, but also create a bounding box around each one that was identified. I've also become proficient compiling, building, and utilizing version of TensoFlow that are most efficient for my system. I believe I would be able to find an outlet for these skills while helping to advance Yembo's vision.
I also have a passion for home design. I have always had a need for symmetry and a Feng shui type attitude towards decorating my own spaces as early as I can remember. No clutter, a natural spacing even in busy environments, creating a flow that was natural and comfortable to my eyes as well as my physical movements throughout the room. Moving houses has always invigorated me as well, knowing that there is a clean-slate just waiting for me somewhere else in the world took a lot of the pain out of lugging all my stuff from one place to the next.
I would love to get caught up on where the team is currently in the project, and determine where in the process I could use my knowledge. I have communicated and worked with various functional areas in my previous roles so networking with different teams and vendors to improve our design or find better sources of data would not be a problem for me.
Long-term I would like to takeover a specific area of the project as my own to improve upon. More precisely the layout of the room and the corresponding items that the algorithm could select for the customer based on their preferences. As I stated before, the spacing and design of rooms are what I naturally gravitate towards, and as I further advance my visual deep learning skills with Yembo, I believe my passions for concise design will not only drive my career, but the success of the company in tandem.
