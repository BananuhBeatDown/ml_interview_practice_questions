1.	We A/B tested two styles for a sign-up button on our company's product page. 100 visitors viewed page A, out of which 20 clicked on the button; whereas, 70 visitors viewed page B, and only 15 of them clicked on the button. Can you confidently say that page A is a better choice, or page B? Why?

	The only way to determine the better button would be to check how many clicks A had when its site was at 70 visitors, or wait to conduct the test once the site has had a total of 100 visitors while B was present. If the determination was made using only the click percentages B would the clear choice since it has a 2% higher click percentage than A (22% v. 20), but again this is based on differening base numerical values for each test. 
	For example, the next 30 people that visit the site while B is present may not click the button at all, and now A is the clear choice since it has a 5% higher click rate than B (20% v 15%). A better solution would be to have longer running tests for both buttons since such small sample sizes can often lead to vague, incorrect, or inconsistent conclusions.


2.	Can you devise a scheme to group Twitter users by looking only at their tweets? No demographic, geographic or other identifying information is available to you, just the messages they’ve posted, in plain text, and a timestamp for each message. Assuming you have a stream of these tweets coming in, describe the process of collecting and analyzing them, what transformations/algorithms you would apply, how you would train and test your model, and present the results.

	Start by inserting the entries into a new dictionary using the user_id as the key and the tweet as the value. Preprocess and normalize all the tweets removing stop words and punctuation marks, stemming words, expanding acronyms, and reducing or eliminating repeat words. The features will then be extracted using engrams and bigrams to capture the words or phrases that will be used during feature selection. Create a new dictionary of the high frequency words or phrases. Train a K-Means clustering algorithm on the new dictionary and test it on newly captured data that has also been preprocessed. Present the results to the interested party in an visual that can be more easliy understood than statistics.


3. In a classification setting, given a dataset of labeled examples and a machine learning model you're trying to fit, describe a strategy to detect and prevent overfitting.

	Detecting overfitting would begin by splitting the dataset into training, validation, and test sets. If the model is performing well on the training, but then failing to obtain the desired accuracy on the validation and test set it is more than likely that the training set is to small or to specific. A good way to address this would be to create a larger test set by skewing the items in the training set and adding them to the original dataset to increase the number of samples and variability in training set.


4. Your team is designing the next generation user experience for your flagship 3D modeling tool. Specifically, you have been tasked with implementing a smart context menu that learns from a modeler’s usage of menu options and shows the ones that would be most beneficial. E.g. I often use Edit > Surface > Smooth Surface, and wish I could just right click and there would be a Smooth Surface option just like Cut, Copy and Paste. Note that not all commands make sense in all contexts, for instance I need to have a surface selected to smooth it. How would you go about designing a learning system/agent to enable this behavior?

	A system of percentage counters could be developed. The setup would take into account what system is actually selected so that only the options that related to the system could be applied to each individually. Once an option is selected a count for that option in that system would be incremented and stored. The system would have a minimum threshold before it could add the feature as a favorite, and a percentage relation to the other options routinely selected to detemine its status. For example an option has to be selected 3 times before it can be even be considered, but if it's used 3 times and another option is used 20 times, the option used 3 times may not break the percentage threshold of the option used 20 times and thus not considered a favorite. This issue could quickly get out of hand and isolate favorites if one option is used numerously while another is used somewhat frequently, so a cap on the times used would have to be considered to allievate the pressure of high use options.















